#Cache

项目介绍
项目描述： 基于C++实现的线程安全的高并发缓存系统，支持多种缓存替换策略（LRU、LFU、ARC）。项目开发注重系统的并发性能优化和缓存策略改进，以提高在高并发场景下的响应速度与命中率。

● 实现了多种缓存替换策略，适配不同的访问模式和业务场景；

● 实现了LRU和LFU的缓存分片，降低锁争用，提升高并发访问下的性能；

● 实现了LRU-k优化，防止热点数据被冷数据替换，减少缓存污染问题；

● 实现了LFU引入最大平均访问频次，淘汰旧的热点数据，提升缓存的整体利用率；

● 实现了ARC策略，动态调整LRU和LFU的权重比例，提升复杂环境下的缓存命中率；

● 通过互斥锁和原子操作实现多线程下的线程安全；

前言：

缓存是将高频访问的数据暂存到内存中，是加速数据访问的存储，降低延迟，提高吞吐率的利器。

缓存可以放在浏览器，反向代理服务器，应用程序进程内，同时可以放在分布式缓存系统中。

这次会开发一个高并发缓存系统，支持多种缓存策略

项目地址：https://github.com/mushanyux/Cache

日志：

day1:

完成LRU在并发状态下的基础实现，使用lock_guard控制锁，采用智能指针管理节点，实现put，get，remove操作

问题：

1.在循环遍历不重复数据时几乎起不到效果

2.不常访问的冷数据可能挤出热数据

3.锁的粒度大

day2：

1.实现LRU-k

原理：继承自基础LRU，另外组合了一个基础LRU记录访问次数，只有访问大于k次的才能进入缓存。避免了被冷数据污染。

2.HashLRU

原理：不好优化锁的粒度，因此采用分片方式来优化，将传入的值哈希后传到对应的分片。这样减小了临界区，提升了并行度，减少同步等待耗时。开一个合适大小的基础LRU数组即可实现。

3.AverageLFU

原理：LFU本身存在一些问题：长期存在缓存中的热数据可能导致计数溢出，过时的热点数据占用缓存，冷启动等问题

在LFU基础上增加了最大平均访问次数限制，当平均值大于最大平均值限制时，将所有节点的访问次数减去一个值，相当于使热点数据老化，这样可以避免计数溢出，缓解缓存污染。

4.HashLFU

原理：类似HashLRU

5.ARC

原理：当访问的数据趋向于访问最近的内容时，会更多地命中LRU list，当系统趋向于访问频繁访问的内容时，会更多的命中LFU list。因此在近期频繁访问和周期性访问交叉的场景之间使用ARC表现较好。

另外对于两个部分都增加了ghost list，存储淘汰的数据。当在缓存中没找到数据，但是在ghost中找到时，调整LRU和LFU的占比，如在LRU的ghost list中找到，就将LRU的占比提升

6.测试

6.1 热点数据访问测试 (testHotDataAccess)

缓存容量：5

操作次数：100,000

数据分布：

40% 访问热点数据（3个热点key）

60% 访问冷数据（5000个key范围）

测试目的：

验证缓存对频繁访问的少量数据的处理能力

测试在大量冷数据干扰下的缓存性能

有利策略：

LFU：因为能准确跟踪访问频率，对热点数据有很好的保留能力

ARC：能够通过自适应机制在T2列表中保留频繁访问的数据

6.2 循环扫描测试 (testLoopPattern)

缓存容量：3

循环范围：200

操作次数：50,000

访问模式：

70% 顺序扫描

15% 随机跳跃

15% 范围外访问

测试目的：

验证缓存在顺序访问模式下的表现

测试缓存对周期性访问模式的适应能力

评估缓存在数据局部性变化时的性能

有利策略：

ARC：通过B1列表能够识别循环访问模式

LRU：在有局部性的顺序访问中表现较好

6.3 工作负载剧烈变化测试 (testWorkloadShift)

测试特征：

缓存容量：4

操作次数：80,000

五个不同阶段：

热点访问（少量key密集访问）

大范围随机访问（1000个key）

顺序扫描（100个key）

局部性随机访问（分区域随机）

混合访问模式

测试目的：

评估缓存在工作负载突变时的适应能力

测试不同缓存策略在复杂访问模式下的表现

验证缓存算法的稳定性和自适应能力

有利策略：

ARC：因其自适应机制能够快速调整到新的访问模式

LRU：在访问模式变化时能较快调整